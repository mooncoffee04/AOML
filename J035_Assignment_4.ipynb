{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T17:59:08.414680Z",
     "iopub.status.busy": "2025-02-08T17:59:08.414190Z",
     "iopub.status.idle": "2025-02-08T17:59:08.588495Z",
     "shell.execute_reply": "2025-02-08T17:59:08.587352Z",
     "shell.execute_reply.started": "2025-02-08T17:59:08.414635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T18:06:49.531801Z",
     "iopub.status.busy": "2025-02-08T18:06:49.531358Z",
     "iopub.status.idle": "2025-02-08T18:06:49.853837Z",
     "shell.execute_reply": "2025-02-08T18:06:49.852668Z",
     "shell.execute_reply.started": "2025-02-08T18:06:49.531766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled successfully!\n",
      "uid                            0\n",
      "day                            0\n",
      "hour                           0\n",
      "minute                         0\n",
      "C_motion                       0\n",
      "feed_water_motion              0\n",
      "faucet_hole                    0\n",
      "vapour_pressure                0\n",
      "vapour_enthalpy                0\n",
      "vapour_pressure_at_division    0\n",
      "vapour_motion                  0\n",
      "feed_water_enth                0\n",
      "vapour_temperature             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('C:/Users/laava/Desktop/sem 6/AOML/train (2).csv')\n",
    "test_df = pd.read_csv('C:/Users/laava/Desktop/sem 6/AOML/test (1).csv')\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['output_electricity_generation'])\n",
    "y_train = train_df['output_electricity_generation']\n",
    "X_test = test_df.copy()  # No target column here\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numerical columns with median\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[numerical_cols] = num_imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = num_imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "# Encode categorical columns for KNN\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "# Impute categorical columns using KNN\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "X_train[categorical_cols] = knn_imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = knn_imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# Convert back to original categorical labels\n",
    "X_train[categorical_cols] = encoder.inverse_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = encoder.inverse_transform(X_test[categorical_cols])\n",
    "\n",
    "print(\"Missing values handled successfully!\")\n",
    "print(X_train.isnull().sum())  # Check if nulls are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T18:07:56.355973Z",
     "iopub.status.busy": "2025-02-08T18:07:56.355522Z",
     "iopub.status.idle": "2025-02-08T18:07:56.404702Z",
     "shell.execute_reply": "2025-02-08T18:07:56.403716Z",
     "shell.execute_reply.started": "2025-02-08T18:07:56.355935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding completed! Shape after encoding:\n",
      "X_train: (50400, 14), X_test: (21600, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Reset index to match original\n",
    "X_train_encoded.index = X_train.index\n",
    "X_test_encoded.index = X_test.index\n",
    "\n",
    "# Drop original categorical columns and merge encoded ones\n",
    "X_train = X_train.drop(columns=categorical_cols).join(X_train_encoded)\n",
    "X_test = X_test.drop(columns=categorical_cols).join(X_test_encoded)\n",
    "\n",
    "print(\"One-hot encoding completed! Shape after encoding:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed: One-Hot Encoding and Scaling done!\n",
      "        uid      hour    minute  C_motion  feed_water_motion  faucet_hole  \\\n",
      "0 -1.732016 -0.860939 -0.012667  0.698866           0.875207     0.606628   \n",
      "1 -1.731948  0.017723 -0.073905 -1.175604          -1.645046     0.732016   \n",
      "2 -1.731879  0.544921 -0.992477  0.082036           0.213797    -0.818066   \n",
      "3 -1.731810  0.369188 -0.012667 -0.972840          -1.608024     0.757067   \n",
      "4 -1.731742  1.072118 -0.808762 -1.614805          -1.922442     1.346672   \n",
      "\n",
      "   vapour_pressure  vapour_enthalpy  vapour_pressure_at_division  \\\n",
      "0         0.579672        -0.728635                     0.593817   \n",
      "1        -1.270916         2.688440                     0.257985   \n",
      "2         0.207561        -0.176404                     0.204875   \n",
      "3        -1.306035         0.400432                    -1.319171   \n",
      "4        -1.700604         0.350202                    -1.719726   \n",
      "\n",
      "   vapour_motion  feed_water_enth  vapour_temperature  day_Friday  \\\n",
      "0       0.660547         0.222118            0.273126         0.0   \n",
      "1      -1.344882        -0.368188            0.345542         0.0   \n",
      "2       0.255390         0.095658            0.293738         1.0   \n",
      "3      -1.313199        -0.376311           -0.175697         0.0   \n",
      "4      -1.557655        -0.551508           -3.121007         0.0   \n",
      "\n",
      "   day_Saturday  day_Friday  day_Saturday  day_nan  \n",
      "0           1.0         0.0           1.0      0.0  \n",
      "1           1.0         0.0           1.0      0.0  \n",
      "2           0.0         1.0           0.0      0.0  \n",
      "3           1.0         0.0           1.0      0.0  \n",
      "4           1.0         0.0           1.0      0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Ensure 'day' column is included in X_train and X_test\n",
    "X_train['day'] = train_df['day']\n",
    "X_test['day'] = test_df['day']\n",
    "\n",
    "# One-Hot Encoding for categorical columns\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_ohe = ohe.fit_transform(X_train[categorical_cols])\n",
    "X_test_ohe = ohe.transform(X_test[categorical_cols])\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "X_test_ohe = pd.DataFrame(X_test_ohe, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Drop original categorical columns and add one-hot encoded columns\n",
    "X_train = X_train.drop(columns=categorical_cols).reset_index(drop=True)\n",
    "X_test = X_test.drop(columns=categorical_cols).reset_index(drop=True)\n",
    "X_train = pd.concat([X_train, X_train_ohe], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_ohe], axis=1)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(\"Preprocessing completed: One-Hot Encoding and Scaling done!\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2646\n",
      "[LightGBM] [Info] Number of data points in the train set: 40320, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 832.300201\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2646\n",
      "[LightGBM] [Info] Number of data points in the train set: 40320, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 832.300201\n",
      "Validation RMSE for XGBoost: 4.4274\n",
      "Validation RMSE for LightGBM: 3.5104\n",
      "Validation RMSE for CatBoost: 4.6362\n",
      "Validation RMSE for Voting Regressor: 3.4747\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# 1️⃣ Split data into train (80%) and validation (20%)\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the input data is in the correct shape\n",
    "X_train_split = X_train_split.values\n",
    "X_val = X_val.values\n",
    "\n",
    "# 2️⃣ Initialize models\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, learning_rate=0.05, random_state=42)\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=42)\n",
    "cat_model = cb.CatBoostRegressor(iterations=500, learning_rate=0.05, depth=6, verbose=0, random_state=42)\n",
    "\n",
    "# 3️⃣ Train models on training split\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "lgb_model.fit(X_train_split, y_train_split)\n",
    "cat_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 4️⃣ Predict on validation set\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "lgb_val_pred = lgb_model.predict(X_val)\n",
    "cat_val_pred = cat_model.predict(X_val)\n",
    "\n",
    "# 5️⃣ Compute RMSE for validation set\n",
    "xgb_val_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_pred))\n",
    "lgb_val_rmse = np.sqrt(mean_squared_error(y_val, lgb_val_pred))\n",
    "cat_val_rmse = np.sqrt(mean_squared_error(y_val, cat_val_pred))\n",
    "\n",
    "# 6️⃣ Voting Regressor (Blended Model)\n",
    "voting_reg = VotingRegressor(estimators=[\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model),\n",
    "    ('cat', cat_model)\n",
    "])\n",
    "voting_reg.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 7️⃣ Get validation predictions from Voting Regressor\n",
    "ensemble_val_pred = voting_reg.predict(X_val)\n",
    "\n",
    "# 8️⃣ Compute RMSE for Voting Regressor on validation set\n",
    "ensemble_val_rmse = np.sqrt(mean_squared_error(y_val, ensemble_val_pred))\n",
    "\n",
    "# 🔥 Print RMSE for validation set\n",
    "print(f\"Validation RMSE for XGBoost: {xgb_val_rmse:.4f}\")\n",
    "print(f\"Validation RMSE for LightGBM: {lgb_val_rmse:.4f}\")\n",
    "print(f\"Validation RMSE for CatBoost: {cat_val_rmse:.4f}\")\n",
    "print(f\"Validation RMSE for Voting Regressor: {ensemble_val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2646\n",
      "[LightGBM] [Info] Number of data points in the train set: 40320, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 832.300201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Final XGBoost Validation RMSE: 2.2135\n",
      "Final LightGBM Validation RMSE: 2.8311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2646\n",
      "[LightGBM] [Info] Number of data points in the train set: 40320, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 832.300201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Final Voting Regressor Validation RMSE: 2.1904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# 1️⃣ Split train-validation set\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the input data is in the correct format\n",
    "X_train_split = X_train_split.values\n",
    "X_val = X_val.values\n",
    "\n",
    "# 2️⃣ Best Parameters from Optuna\n",
    "best_xgb_params = {\n",
    "    \"n_estimators\": 790,\n",
    "    \"max_depth\": 11,\n",
    "    \"learning_rate\": 0.0879951300409359,\n",
    "    \"subsample\": 0.8168097123116692,\n",
    "    \"colsample_bytree\": 0.8391529700118769,\n",
    "    \"reg_lambda\": 0.0905254702059362,\n",
    "    \"reg_alpha\": 8.71874092645817,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "best_lgb_params = {\n",
    "    \"n_estimators\": 641,\n",
    "    \"num_leaves\": 64,\n",
    "    \"learning_rate\": 0.16236033385267262,\n",
    "    \"subsample\": 0.9985436628526456,\n",
    "    \"colsample_bytree\": 0.9516585974076679,\n",
    "    \"reg_lambda\": 5.480182346492727,\n",
    "    \"reg_alpha\": 0.041685890523880054,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# 3️⃣ Train Final Models with Best Params\n",
    "xgb_model = xgb.XGBRegressor(**best_xgb_params)\n",
    "lgb_model = lgb.LGBMRegressor(**best_lgb_params)\n",
    "\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "lgb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 4️⃣ Evaluate Final Models on Validation Set\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "lgb_val_pred = lgb_model.predict(X_val)\n",
    "\n",
    "xgb_val_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_pred))\n",
    "lgb_val_rmse = np.sqrt(mean_squared_error(y_val, lgb_val_pred))\n",
    "\n",
    "print(f\"Final XGBoost Validation RMSE: {xgb_val_rmse:.4f}\")\n",
    "print(f\"Final LightGBM Validation RMSE: {lgb_val_rmse:.4f}\")\n",
    "\n",
    "# 5️⃣ Train Updated Voting Regressor\n",
    "voting_reg = VotingRegressor(estimators=[\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "])\n",
    "voting_reg.fit(X_train_split, y_train_split)\n",
    "\n",
    "ensemble_val_pred = voting_reg.predict(X_val)\n",
    "ensemble_val_rmse = np.sqrt(mean_squared_error(y_val, ensemble_val_pred))\n",
    "\n",
    "print(f\"Final Voting Regressor Validation RMSE: {ensemble_val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2646\n",
      "[LightGBM] [Info] Number of data points in the train set: 50400, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 831.808573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2646\n",
      "[LightGBM] [Info] Number of data points in the train set: 50400, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 831.808573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "✅ Predictions saved with uid as 'final_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input data is in the correct shape\n",
    "X_train_values = X_train.values\n",
    "X_test_values = X_test.values\n",
    "\n",
    "# Train the final models on FULL training data\n",
    "xgb_model.fit(X_train_values, y_train)\n",
    "lgb_model.fit(X_train_values, y_train)\n",
    "cat_model.fit(X_train_values, y_train)\n",
    "\n",
    "# Train the Voting Regressor on full training data\n",
    "voting_reg.fit(X_train_values, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "test_preds = voting_reg.predict(X_test_values)\n",
    "\n",
    "# Create submission dataframe with uid and predictions\n",
    "submission_df = test_df[[\"uid\"]].copy()\n",
    "submission_df[\"output_electricity_generation\"] = test_preds\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(\"submissions3.csv\", index=False)\n",
    "\n",
    "print(\"✅ Predictions saved with uid as 'final_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11034314,
     "sourceId": 92582,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
